{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Throughout this notebook I use the paper _\"From reads to insight: a hitchhiker’s guide to ATAC-seq data analysis\"_ as a guide. It was published in February this year and gives a detailed description of the ATAC-seq workflow, pointing out potential pitfalls and describing the different tools which can be used at each stage in the pipeline. Here is a [link](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-1929-3) to the paper. \n",
    "\n",
    "Eventually, I decided to stick with the pipeline given in the reference notebook, but I considered other tools at each stage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p /mnt/storage/$USER/jupyternotebooks/epigenomics/ATAC/\n",
    "cd /mnt/storage/$USER/jupyternotebooks/epigenomics/ATAC/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdb-config -s /repository/user/cache-disabled=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-08T15:20:38 fastq-dump.2.9.6 sys: timeout exhausted while reading file within network system module - mbedtls_ssl_read returned -76 ( NET - Reading information from the socket failed )\n",
      "2020-12-08T15:21:47 fastq-dump.2.9.6 sys: timeout exhausted while reading file within network system module - mbedtls_ssl_read returned -76 ( NET - Reading information from the socket failed )\n",
      "2020-12-08T15:25:20 fastq-dump.2.9.6 sys: timeout exhausted while reading file within network system module - mbedtls_ssl_read returned -76 ( NET - Reading information from the socket failed )\n"
     ]
    }
   ],
   "source": [
    "fastq-dump --split-files SRR9113345 SRR9113348"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Control\n",
    "##  Pre-alignment quality control using FastQC\n",
    "I run FastQC to check the quality of the reads. In the previous assignment I discussed each of the tests in detail so I won't do that here. Due to the ubiquitous use of Illumina’s Nextera library for ATAC-seq, overrepresentation of Nextera sequencing adapters is often observed and should be removed for accurate read alignment. The FastQC reports showed that each run failed the `per base sequence content` and `kmer content` tests. The QC report indicated that the quality of the reads were fine and the reads didn't need to be trimmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started analysis of SRR9113339_1.fastq\n",
      "Approx 5% complete for SRR9113339_1.fastq\n",
      "Approx 10% complete for SRR9113339_1.fastq\n",
      "Approx 15% complete for SRR9113339_1.fastq\n",
      "Approx 20% complete for SRR9113339_1.fastq\n",
      "Approx 25% complete for SRR9113339_1.fastq\n",
      "Approx 30% complete for SRR9113339_1.fastq\n",
      "Approx 35% complete for SRR9113339_1.fastq\n",
      "Approx 40% complete for SRR9113339_1.fastq\n",
      "Approx 45% complete for SRR9113339_1.fastq\n",
      "Approx 50% complete for SRR9113339_1.fastq\n",
      "Approx 55% complete for SRR9113339_1.fastq\n",
      "Approx 60% complete for SRR9113339_1.fastq\n",
      "Approx 65% complete for SRR9113339_1.fastq\n",
      "Approx 70% complete for SRR9113339_1.fastq\n",
      "Approx 75% complete for SRR9113339_1.fastq\n",
      "Approx 80% complete for SRR9113339_1.fastq\n",
      "Approx 85% complete for SRR9113339_1.fastq\n",
      "Approx 90% complete for SRR9113339_1.fastq\n",
      "Approx 95% complete for SRR9113339_1.fastq\n",
      "Analysis complete for SRR9113339_1.fastq\n",
      "Started analysis of SRR9113339_2.fastq\n",
      "Approx 5% complete for SRR9113339_2.fastq\n",
      "Approx 10% complete for SRR9113339_2.fastq\n",
      "Approx 15% complete for SRR9113339_2.fastq\n",
      "Approx 20% complete for SRR9113339_2.fastq\n",
      "Approx 25% complete for SRR9113339_2.fastq\n",
      "Approx 30% complete for SRR9113339_2.fastq\n",
      "Approx 35% complete for SRR9113339_2.fastq\n",
      "Approx 40% complete for SRR9113339_2.fastq\n",
      "Approx 45% complete for SRR9113339_2.fastq\n",
      "Approx 50% complete for SRR9113339_2.fastq\n",
      "Approx 55% complete for SRR9113339_2.fastq\n",
      "Approx 60% complete for SRR9113339_2.fastq\n",
      "Approx 65% complete for SRR9113339_2.fastq\n",
      "Approx 70% complete for SRR9113339_2.fastq\n",
      "Approx 75% complete for SRR9113339_2.fastq\n",
      "Approx 80% complete for SRR9113339_2.fastq\n",
      "Approx 85% complete for SRR9113339_2.fastq\n",
      "Approx 90% complete for SRR9113339_2.fastq\n",
      "Approx 95% complete for SRR9113339_2.fastq\n",
      "Analysis complete for SRR9113339_2.fastq\n",
      "Started analysis of SRR9113342_1.fastq\n",
      "Approx 5% complete for SRR9113342_1.fastq\n",
      "Approx 10% complete for SRR9113342_1.fastq\n",
      "Approx 15% complete for SRR9113342_1.fastq\n",
      "Approx 20% complete for SRR9113342_1.fastq\n",
      "Approx 25% complete for SRR9113342_1.fastq\n",
      "Approx 30% complete for SRR9113342_1.fastq\n",
      "Approx 35% complete for SRR9113342_1.fastq\n",
      "Approx 40% complete for SRR9113342_1.fastq\n",
      "Approx 45% complete for SRR9113342_1.fastq\n",
      "Approx 50% complete for SRR9113342_1.fastq\n",
      "Approx 55% complete for SRR9113342_1.fastq\n",
      "Approx 60% complete for SRR9113342_1.fastq\n",
      "Approx 65% complete for SRR9113342_1.fastq\n",
      "Approx 70% complete for SRR9113342_1.fastq\n",
      "Approx 75% complete for SRR9113342_1.fastq\n",
      "Approx 80% complete for SRR9113342_1.fastq\n",
      "Approx 85% complete for SRR9113342_1.fastq\n",
      "Approx 90% complete for SRR9113342_1.fastq\n",
      "Approx 95% complete for SRR9113342_1.fastq\n",
      "Analysis complete for SRR9113342_1.fastq\n",
      "Started analysis of SRR9113342_2.fastq\n",
      "Approx 5% complete for SRR9113342_2.fastq\n",
      "Approx 10% complete for SRR9113342_2.fastq\n",
      "Approx 15% complete for SRR9113342_2.fastq\n",
      "Approx 20% complete for SRR9113342_2.fastq\n",
      "Approx 25% complete for SRR9113342_2.fastq\n",
      "Approx 30% complete for SRR9113342_2.fastq\n",
      "Approx 35% complete for SRR9113342_2.fastq\n",
      "Approx 40% complete for SRR9113342_2.fastq\n",
      "Approx 45% complete for SRR9113342_2.fastq\n",
      "Approx 50% complete for SRR9113342_2.fastq\n",
      "Approx 55% complete for SRR9113342_2.fastq\n",
      "Approx 60% complete for SRR9113342_2.fastq\n",
      "Approx 65% complete for SRR9113342_2.fastq\n",
      "Approx 70% complete for SRR9113342_2.fastq\n",
      "Approx 75% complete for SRR9113342_2.fastq\n",
      "Approx 80% complete for SRR9113342_2.fastq\n",
      "Approx 85% complete for SRR9113342_2.fastq\n",
      "Approx 90% complete for SRR9113342_2.fastq\n",
      "Approx 95% complete for SRR9113342_2.fastq\n",
      "Analysis complete for SRR9113342_2.fastq\n",
      "Started analysis of SRR9113345_1.fastq\n",
      "Approx 5% complete for SRR9113345_1.fastq\n",
      "Approx 10% complete for SRR9113345_1.fastq\n",
      "Approx 15% complete for SRR9113345_1.fastq\n",
      "Approx 20% complete for SRR9113345_1.fastq\n",
      "Approx 25% complete for SRR9113345_1.fastq\n",
      "Approx 30% complete for SRR9113345_1.fastq\n",
      "Approx 35% complete for SRR9113345_1.fastq\n",
      "Approx 40% complete for SRR9113345_1.fastq\n",
      "Approx 45% complete for SRR9113345_1.fastq\n",
      "Approx 50% complete for SRR9113345_1.fastq\n",
      "Approx 55% complete for SRR9113345_1.fastq\n",
      "Approx 60% complete for SRR9113345_1.fastq\n",
      "Approx 65% complete for SRR9113345_1.fastq\n",
      "Approx 70% complete for SRR9113345_1.fastq\n",
      "Approx 75% complete for SRR9113345_1.fastq\n",
      "Approx 80% complete for SRR9113345_1.fastq\n",
      "Approx 85% complete for SRR9113345_1.fastq\n",
      "Approx 90% complete for SRR9113345_1.fastq\n",
      "Approx 95% complete for SRR9113345_1.fastq\n",
      "Analysis complete for SRR9113345_1.fastq\n",
      "Started analysis of SRR9113345_2.fastq\n",
      "Approx 5% complete for SRR9113345_2.fastq\n",
      "Approx 10% complete for SRR9113345_2.fastq\n",
      "Approx 15% complete for SRR9113345_2.fastq\n",
      "Approx 20% complete for SRR9113345_2.fastq\n",
      "Approx 25% complete for SRR9113345_2.fastq\n",
      "Approx 30% complete for SRR9113345_2.fastq\n",
      "Approx 35% complete for SRR9113345_2.fastq\n",
      "Approx 40% complete for SRR9113345_2.fastq\n",
      "Approx 45% complete for SRR9113345_2.fastq\n",
      "Approx 50% complete for SRR9113345_2.fastq\n",
      "Approx 55% complete for SRR9113345_2.fastq\n",
      "Approx 60% complete for SRR9113345_2.fastq\n",
      "Approx 65% complete for SRR9113345_2.fastq\n",
      "Approx 70% complete for SRR9113345_2.fastq\n",
      "Approx 75% complete for SRR9113345_2.fastq\n",
      "Approx 80% complete for SRR9113345_2.fastq\n",
      "Approx 85% complete for SRR9113345_2.fastq\n",
      "Approx 90% complete for SRR9113345_2.fastq\n",
      "Approx 95% complete for SRR9113345_2.fastq\n",
      "Analysis complete for SRR9113345_2.fastq\n",
      "Started analysis of SRR9113348_1.fastq\n",
      "Approx 5% complete for SRR9113348_1.fastq\n",
      "Approx 10% complete for SRR9113348_1.fastq\n",
      "Approx 15% complete for SRR9113348_1.fastq\n",
      "Approx 20% complete for SRR9113348_1.fastq\n",
      "Approx 25% complete for SRR9113348_1.fastq\n",
      "Approx 30% complete for SRR9113348_1.fastq\n",
      "Approx 35% complete for SRR9113348_1.fastq\n",
      "Approx 40% complete for SRR9113348_1.fastq\n",
      "Approx 45% complete for SRR9113348_1.fastq\n",
      "Approx 50% complete for SRR9113348_1.fastq\n",
      "Approx 55% complete for SRR9113348_1.fastq\n",
      "Approx 60% complete for SRR9113348_1.fastq\n",
      "Approx 65% complete for SRR9113348_1.fastq\n",
      "Approx 70% complete for SRR9113348_1.fastq\n",
      "Approx 75% complete for SRR9113348_1.fastq\n",
      "Approx 80% complete for SRR9113348_1.fastq\n",
      "Approx 85% complete for SRR9113348_1.fastq\n",
      "Approx 90% complete for SRR9113348_1.fastq\n",
      "Approx 95% complete for SRR9113348_1.fastq\n",
      "Analysis complete for SRR9113348_1.fastq\n",
      "Started analysis of SRR9113348_2.fastq\n",
      "Approx 5% complete for SRR9113348_2.fastq\n",
      "Approx 10% complete for SRR9113348_2.fastq\n",
      "Approx 15% complete for SRR9113348_2.fastq\n",
      "Approx 20% complete for SRR9113348_2.fastq\n",
      "Approx 25% complete for SRR9113348_2.fastq\n",
      "Approx 30% complete for SRR9113348_2.fastq\n",
      "Approx 35% complete for SRR9113348_2.fastq\n",
      "Approx 40% complete for SRR9113348_2.fastq\n",
      "Approx 45% complete for SRR9113348_2.fastq\n",
      "Approx 50% complete for SRR9113348_2.fastq\n",
      "Approx 55% complete for SRR9113348_2.fastq\n",
      "Approx 60% complete for SRR9113348_2.fastq\n",
      "Approx 65% complete for SRR9113348_2.fastq\n",
      "Approx 70% complete for SRR9113348_2.fastq\n",
      "Approx 75% complete for SRR9113348_2.fastq\n",
      "Approx 80% complete for SRR9113348_2.fastq\n",
      "Approx 85% complete for SRR9113348_2.fastq\n",
      "Approx 90% complete for SRR9113348_2.fastq\n",
      "Approx 95% complete for SRR9113348_2.fastq\n",
      "Analysis complete for SRR9113348_2.fastq\n"
     ]
    }
   ],
   "source": [
    "/usr/bin/fastqc -o . *.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment\n",
    "Alignment was done using Bowtie2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34676467 reads; of these:\n",
      "  34676467 (100.00%) were paired; of these:\n",
      "    2715124 (7.83%) aligned concordantly 0 times\n",
      "    19068002 (54.99%) aligned concordantly exactly 1 time\n",
      "    12893341 (37.18%) aligned concordantly >1 times\n",
      "    ----\n",
      "    2715124 pairs aligned concordantly 0 times; of these:\n",
      "      798052 (29.39%) aligned discordantly 1 time\n",
      "    ----\n",
      "    1917072 pairs aligned 0 times concordantly or discordantly; of these:\n",
      "      3834144 mates make up the pairs; of these:\n",
      "        2362395 (61.61%) aligned 0 times\n",
      "        596813 (15.57%) aligned exactly 1 time\n",
      "        874936 (22.82%) aligned >1 times\n",
      "96.59% overall alignment rate\n",
      "29428293 reads; of these:\n",
      "  29428293 (100.00%) were paired; of these:\n",
      "    2881950 (9.79%) aligned concordantly 0 times\n",
      "    18949636 (64.39%) aligned concordantly exactly 1 time\n",
      "    7596707 (25.81%) aligned concordantly >1 times\n",
      "    ----\n",
      "    2881950 pairs aligned concordantly 0 times; of these:\n",
      "      756404 (26.25%) aligned discordantly 1 time\n",
      "    ----\n",
      "    2125546 pairs aligned 0 times concordantly or discordantly; of these:\n",
      "      4251092 mates make up the pairs; of these:\n",
      "        3090768 (72.71%) aligned 0 times\n",
      "        497283 (11.70%) aligned exactly 1 time\n",
      "        663041 (15.60%) aligned >1 times\n",
      "94.75% overall alignment rate\n",
      "27169796 reads; of these:\n",
      "  27169796 (100.00%) were paired; of these:\n",
      "    3698918 (13.61%) aligned concordantly 0 times\n",
      "    20138882 (74.12%) aligned concordantly exactly 1 time\n",
      "    3331996 (12.26%) aligned concordantly >1 times\n",
      "    ----\n",
      "    3698918 pairs aligned concordantly 0 times; of these:\n",
      "      928029 (25.09%) aligned discordantly 1 time\n",
      "    ----\n",
      "    2770889 pairs aligned 0 times concordantly or discordantly; of these:\n",
      "      5541778 mates make up the pairs; of these:\n",
      "        4270200 (77.05%) aligned 0 times\n",
      "        555788 (10.03%) aligned exactly 1 time\n",
      "        715790 (12.92%) aligned >1 times\n",
      "92.14% overall alignment rate\n",
      "28404404 reads; of these:\n",
      "  28404404 (100.00%) were paired; of these:\n",
      "    2617470 (9.22%) aligned concordantly 0 times\n",
      "    16823147 (59.23%) aligned concordantly exactly 1 time\n",
      "    8963787 (31.56%) aligned concordantly >1 times\n",
      "    ----\n",
      "    2617470 pairs aligned concordantly 0 times; of these:\n",
      "      790241 (30.19%) aligned discordantly 1 time\n",
      "    ----\n",
      "    1827229 pairs aligned 0 times concordantly or discordantly; of these:\n",
      "      3654458 mates make up the pairs; of these:\n",
      "        2458682 (67.28%) aligned 0 times\n",
      "        502884 (13.76%) aligned exactly 1 time\n",
      "        692892 (18.96%) aligned >1 times\n",
      "95.67% overall alignment rate\n"
     ]
    }
   ],
   "source": [
    "bowtie2 -x /mnt/storage/data/resources/bowtie2/hg19 -1 SRR9113339_1.fastq -2 SRR9113339_2.fastq -S ATAC_DMSO.sam\n",
    "bowtie2 -x /mnt/storage/data/resources/bowtie2/hg19 -1 SRR9113342_1.fastq -2 SRR9113342_2.fastq -S ATAC_O.sam\n",
    "bowtie2 -x /mnt/storage/data/resources/bowtie2/hg19 -1 SRR9113345_1.fastq -2 SRR9113345_2.fastq -S ATAC_OT.sam\n",
    "bowtie2 -x /mnt/storage/data/resources/bowtie2/hg19 -1 SRR9113348_1.fastq -2 SRR9113348_2.fastq -S ATAC_rebound.sam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to Bam file and create an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bam_sort_core] merging from 16 files and 1 in-memory blocks...\n",
      "[bam_sort_core] merging from 13 files and 1 in-memory blocks...\n",
      "[bam_sort_core] merging from 12 files and 1 in-memory blocks...\n",
      "[bam_sort_core] merging from 13 files and 1 in-memory blocks...\n"
     ]
    }
   ],
   "source": [
    "samtools sort -o ATAC_DMSO.bam ATAC_DMSO.sam\n",
    "samtools sort -o ATAC_O.bam ATAC_O.sam\n",
    "samtools sort -o ATAC_OT.bam ATAC_OT.sam\n",
    "samtools sort -o ATAC_rebound.bam ATAC_rebound.sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "samtools index ATAC_DMSO.bam\n",
    "samtools index ATAC_O.bam\n",
    "samtools index ATAC_OT.bam\n",
    "samtools index ATAC_rebound.bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-alignment processing and quality control\n",
    "After sequence alignment, as in most DNA sequencing data, basic metrics of the aligned BAM file, such as unique mapping reads/rates, duplicated read percentages, and fragment size distribution can be collected using Picard and SAMtools. Reads should be removed if they are improperly paired or of low mapping quality. Additionally, the mitochondrial genome, which is more accessible due to the lack of chromatin packaging, and the ENCODE blacklisted regions often have extremely high read coverage, and should also be discarded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall alignment rates for the four conditions were 97%, 95%, 92% and 95%. First I used `samtools` to look at basic statistics, which looked normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1\t249250621\t5774239\t30283\n",
      "chr2\t243199373\t2984561\t20596\n",
      "chr3\t198022430\t2274993\t12130\n",
      "chr4\t191154276\t1414891\t8768\n",
      "chr5\t180915260\t2556082\t15641\n",
      "chr6\t171115067\t1762962\t9624\n",
      "chr7\t159138663\t2567996\t14552\n",
      "chr8\t146364022\t1317030\t8183\n",
      "chr9\t141213431\t1522265\t9110\n",
      "chr10\t135534747\t1448691\t10194\n",
      "chr11\t135006516\t2186417\t13956\n",
      "chr12\t133851895\t1634793\t9263\n",
      "chr13\t115169878\t673477\t3968\n",
      "chr14\t107349540\t1002588\t5754\n",
      "chr15\t102531392\t1031752\t5848\n",
      "chr16\t90354753\t1494110\t8240\n",
      "chr17\t81195210\t1768429\t11851\n",
      "chr18\t78077248\t470133\t3087\n",
      "chr19\t59128983\t1448025\t9156\n",
      "chr20\t63025520\t1005243\t5453\n",
      "chr21\t48129895\t316525\t2252\n",
      "chr22\t51304566\t772098\t4242\n",
      "chrX\t155270560\t815024\t6173\n",
      "chrY\t59373566\t43122\t1511\n",
      "chrM\t16571\t28511826\t163549\n",
      "chr1_gl000191_random\t106433\t1809\t13\n",
      "chr1_gl000192_random\t547496\t5560\t25\n",
      "chr4_gl000193_random\t189789\t1631\t25\n",
      "chr4_gl000194_random\t191469\t1588\t10\n",
      "chr7_gl000195_random\t182896\t11400\t50\n",
      "chr8_gl000196_random\t38914\t103\t1\n",
      "chr8_gl000197_random\t37175\t154\t0\n",
      "chr9_gl000198_random\t90085\t502\t13\n",
      "chr9_gl000199_random\t169874\t8678\t464\n",
      "chr9_gl000200_random\t187035\t274\t2\n",
      "chr9_gl000201_random\t36148\t562\t5\n",
      "chr11_gl000202_random\t40103\t1516\t5\n",
      "chr17_gl000203_random\t37498\t59\t1\n",
      "chr17_gl000204_random\t81310\t648\t4\n",
      "chr17_gl000205_random\t174588\t10877\t97\n",
      "chr17_gl000206_random\t41001\t808\t7\n",
      "chr18_gl000207_random\t4262\t5\t1\n",
      "chr19_gl000208_random\t92689\t811\t101\n",
      "chr19_gl000209_random\t159169\t468\t1\n",
      "chr21_gl000210_random\t27682\t409\t4\n",
      "chrUn_gl000211\t166566\t998\t12\n",
      "chrUn_gl000212\t186858\t1299\t8\n",
      "chrUn_gl000213\t164239\t446\t10\n",
      "chrUn_gl000214\t137718\t1294\t10\n",
      "chrUn_gl000215\t172545\t381\t6\n",
      "chrUn_gl000216\t172294\t3263\t434\n",
      "chrUn_gl000217\t172149\t918\t8\n",
      "chrUn_gl000218\t161147\t959\t9\n",
      "chrUn_gl000219\t179198\t3303\t92\n",
      "chrUn_gl000220\t161802\t106676\t871\n",
      "chrUn_gl000221\t155397\t776\t2\n",
      "chrUn_gl000222\t186861\t1380\t23\n",
      "chrUn_gl000223\t180455\t1530\t6\n",
      "chrUn_gl000224\t179693\t4102\t28\n",
      "chrUn_gl000225\t211173\t7333\t665\n",
      "chrUn_gl000226\t15008\t2903\t80\n",
      "chrUn_gl000227\t128374\t909\t5\n",
      "chrUn_gl000228\t129120\t1816\t95\n",
      "chrUn_gl000229\t19913\t499\t11\n",
      "chrUn_gl000230\t43691\t41\t2\n",
      "chrUn_gl000231\t27386\t279\t8\n",
      "chrUn_gl000232\t40652\t335\t7\n",
      "chrUn_gl000233\t45941\t68\t4\n",
      "chrUn_gl000234\t40531\t519\t10\n",
      "chrUn_gl000235\t34474\t160\t7\n",
      "chrUn_gl000236\t41934\t83\t1\n",
      "chrUn_gl000237\t45867\t243\t3\n",
      "chrUn_gl000238\t39939\t67\t0\n",
      "chrUn_gl000239\t33824\t174\t2\n",
      "chrUn_gl000240\t41933\t370\t1\n",
      "chrUn_gl000241\t42152\t512\t11\n",
      "chrUn_gl000242\t43523\t291\t1\n",
      "chrUn_gl000243\t43341\t609\t7\n",
      "chrUn_gl000244\t39929\t110\t0\n",
      "chrUn_gl000245\t36651\t91\t1\n",
      "chrUn_gl000246\t38154\t35\t0\n",
      "chrUn_gl000247\t36422\t210\t1\n",
      "chrUn_gl000248\t39786\t331\t3\n",
      "chrUn_gl000249\t38502\t92\t4\n",
      "*\t0\t0\t1965734\n"
     ]
    }
   ],
   "source": [
    "samtools idxstats ATAC_DMSO.bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69352934 + 0 in total (QC-passed reads + QC-failed reads)\n",
      "0 + 0 secondary\n",
      "0 + 0 supplementary\n",
      "0 + 0 duplicates\n",
      "66990539 + 0 mapped (96.59% : N/A)\n",
      "69352934 + 0 paired in sequencing\n",
      "34676467 + 0 read1\n",
      "34676467 + 0 read2\n",
      "63922686 + 0 properly paired (92.17% : N/A)\n",
      "66593878 + 0 with itself and mate mapped\n",
      "396661 + 0 singletons (0.57% : N/A)\n",
      "642388 + 0 with mate mapped to a different chr\n",
      "275037 + 0 with mate mapped to a different chr (mapQ>=5)\n",
      "58856586 + 0 in total (QC-passed reads + QC-failed reads)\n",
      "0 + 0 secondary\n",
      "0 + 0 supplementary\n",
      "0 + 0 duplicates\n",
      "55765818 + 0 mapped (94.75% : N/A)\n",
      "58856586 + 0 paired in sequencing\n",
      "29428293 + 0 read1\n",
      "29428293 + 0 read2\n",
      "53092686 + 0 properly paired (90.21% : N/A)\n",
      "55446294 + 0 with itself and mate mapped\n",
      "319524 + 0 singletons (0.54% : N/A)\n",
      "475648 + 0 with mate mapped to a different chr\n",
      "205692 + 0 with mate mapped to a different chr (mapQ>=5)\n",
      "54339592 + 0 in total (QC-passed reads + QC-failed reads)\n",
      "0 + 0 secondary\n",
      "0 + 0 supplementary\n",
      "0 + 0 duplicates\n",
      "50069392 + 0 mapped (92.14% : N/A)\n",
      "54339592 + 0 paired in sequencing\n",
      "27169796 + 0 read1\n",
      "27169796 + 0 read2\n",
      "46941756 + 0 properly paired (86.39% : N/A)\n",
      "49751582 + 0 with itself and mate mapped\n",
      "317810 + 0 singletons (0.58% : N/A)\n",
      "528284 + 0 with mate mapped to a different chr\n",
      "231980 + 0 with mate mapped to a different chr (mapQ>=5)\n",
      "56808808 + 0 in total (QC-passed reads + QC-failed reads)\n",
      "0 + 0 secondary\n",
      "0 + 0 supplementary\n",
      "0 + 0 duplicates\n",
      "54350126 + 0 mapped (95.67% : N/A)\n",
      "56808808 + 0 paired in sequencing\n",
      "28404404 + 0 read1\n",
      "28404404 + 0 read2\n",
      "51573868 + 0 properly paired (90.78% : N/A)\n",
      "54038384 + 0 with itself and mate mapped\n",
      "311742 + 0 singletons (0.55% : N/A)\n",
      "488664 + 0 with mate mapped to a different chr\n",
      "212523 + 0 with mate mapped to a different chr (mapQ>=5)\n"
     ]
    }
   ],
   "source": [
    "samtools flagstat ATAC_DMSO.bam\n",
    "samtools flagstat ATAC_O.bam\n",
    "samtools flagstat ATAC_OT.bam\n",
    "samtools flagstat ATAC_rebound.bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are additional ATAC-seq-specific quality metrics that should also be evaluated. Typically, a successful ATAC-seq experiment should generate a fragment size distribution plot with decreasing and periodical peaks corresponding to the nucleosome-free regions (NFR). Fragments from the NFR are expected to be enriched around the transcription start site (TSS) of genes, while fragments from nucleosome-bound regions are expected to be depleted at TSS with a slight enrichment of flanking regions around TSS. These can be evaluated with the R package ATACseqQC. ATACseqQC can also be used to filter the mitochondrial genome and ENCODE blacklisted regions. Lastly, reads should be shifted + 4 bp and − 5 bp for positive and negative strand respectively, to account for the 9-bp duplication created by DNA repair of the nick by Tn5 transposase and achieve base-pair resolution of TF footprint and motif-related analyses.\n",
    "\n",
    "The ATACseqQC package is not installed on the server so I tried to run this analysis locally but it was too computationally intensive. Therefore I don't have any results to present but I wanted to show that I had looked into this and learned from it at least.\n",
    "\n",
    "DeepTools can also be used to perform some basic quality control, as well as look at PCA plots and heatmaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bins found: 310124\n"
     ]
    }
   ],
   "source": [
    "multiBamSummary bins --bamfiles ATAC_DMSO.bam ATAC_O.bam ATAC_OT.bam ATAC_rebound.bam -o readCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPCA -in readCounts \\\n",
    "-o PCA_readCounts.png \\\n",
    "-T \"PCA of read counts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA plot shows that the DMSO, Osimertinib, and rebound are all similar and that the chromatin in cells treated with osimertinib and trametinib is different. \n",
    "![PCA](images/PCA_readCounts.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total/filtered/left: 310124/7/310117\n",
      "\n",
      "Outliers were detected in the data. They will be removed to avoid bias in the pearson correlation.\n"
     ]
    }
   ],
   "source": [
    "plotCorrelation --corData readCounts \\\n",
    "                --corMethod pearson  \\\n",
    "                --whatToPlot heatmap \\\n",
    "                --removeOutliers     \\\n",
    "                -o heatmap-pearson.png\n",
    "                \n",
    "plotCorrelation --corData readCounts \\\n",
    "                --corMethod spearman \\\n",
    "                --whatToPlot heatmap \\\n",
    "                -o heatmap-spearman.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A heatmap was plotted using Pearson correlation shich shows again the similarity between different conditions. Again we see that OT is most different from the other three conditions which are similar. The correlation plot shows that REBOUND and DMSO are most similar to each other while OT is the least similar to all other three conditions. This fits with the findings in the paper that OT treated cells have a distinct chromatin state which reverses to its original upon state upon washout.\n",
    "\n",
    "![heatmap-pearson](images/heatmap-pearson.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total/filtered/left: 310124/7/310117\n",
      "\n",
      "Outliers were detected in the data. They will be removed to avoid bias in the pearson correlation.\n"
     ]
    }
   ],
   "source": [
    "plotCorrelation --corData readCounts \\\n",
    "                --corMethod pearson  \\\n",
    "                --whatToPlot scatterplot \\\n",
    "                --removeOutliers     \\\n",
    "                -o scatter-pearson.png\n",
    "                \n",
    "plotCorrelation --corData readCounts \\\n",
    "                --corMethod spearman \\\n",
    "                --whatToPlot scatterplot \\\n",
    "                -o scatter-spearman.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plots tell a similar story.\n",
    "![scatter-pearson](images/scatter-pearson.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Analysis: Peak Calling\n",
    "I perform peak calling using MACS2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  @ Thu, 10 Dec 2020 11:57:49: # read alignment files... \n",
      "INFO  @ Thu, 10 Dec 2020 11:57:49: # read treatment tags... \n",
      "INFO  @ Thu, 10 Dec 2020 11:57:49: Detected format is: BAM \n",
      "INFO  @ Thu, 10 Dec 2020 11:57:49: * Input file is gzipped. \n",
      "INFO  @ Thu, 10 Dec 2020 11:57:51:  1000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:57:53:  2000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:57:54:  3000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:57:56:  4000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:57:58:  5000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:00:  6000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:02:  7000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:03:  8000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:05:  9000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:07:  10000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:09:  11000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:10:  12000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:12:  13000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:14:  14000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:16:  15000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:18:  16000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:19:  17000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:21:  18000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:23:  19000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:25:  20000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:26:  21000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:28:  22000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:30:  23000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:32:  24000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:34:  25000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:35:  26000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:37:  27000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:39:  28000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:41:  29000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:42:  30000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:44:  31000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:46:  32000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:48:  33000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:49:  34000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:51:  35000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:53:  36000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:55:  37000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:56:  38000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:58:58:  39000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:00:  40000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:01:  41000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:03:  42000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:04:  43000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:06:  44000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:08:  45000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:09:  46000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:11:  47000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:12:  48000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:14:  49000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:16:  50000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:17:  51000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:19:  52000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:20:  53000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:22:  54000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:23:  55000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:25:  56000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:27:  57000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:28:  58000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:30:  59000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:31:  60000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:33:  61000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:35:  62000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:36:  63000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:38:  64000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:39:  65000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:41:  66000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:43:  67000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:44:  68000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:45:  69000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:46: tag size is determined as 35 bps \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:46: # tag size = 35 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:46: # total tags in alignment file: 31961343 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:46: # Build Peak Model... \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:46: #2 looking for paired plus/minus strand peaks... \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:51: #2 number of paired peaks: 71369 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:51: start model_add_line... \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:52: start X-correlation... \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:52: end of X-cor \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:52: # finished! \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:52: # predicted fragment length is 109 bps \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:52: # alternative fragment length(s) may be 109 bps \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:52: # Generate R script for model : predictd \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:52: # read alignment files... \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:52: # read treatment tags... \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:52: Detected format is: BAM \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:52: * Input file is gzipped. \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:54:  1000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:56:  2000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:58:  3000000 \n",
      "INFO  @ Thu, 10 Dec 2020 11:59:59:  4000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:01:  5000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:03:  6000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:05:  7000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:06:  8000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:08:  9000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:10:  10000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:12:  11000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:13:  12000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:15:  13000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:17:  14000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:19:  15000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:20:  16000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:22:  17000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:24:  18000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:26:  19000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:27:  20000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:29:  21000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:31:  22000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:33:  23000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:34:  24000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:36:  25000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:38:  26000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:40:  27000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:41:  28000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:43:  29000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:45:  30000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:47:  31000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:49:  32000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:51:  33000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:52:  34000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:54:  35000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:56:  36000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:00:58:  37000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:00:  38000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:01:  39000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:03:  40000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:05:  41000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:07:  42000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:08:  43000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:10:  44000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:12:  45000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:13:  46000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:15:  47000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:16:  48000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:18:  49000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:20:  50000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:21:  51000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:23:  52000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:24:  53000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:26:  54000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:28:  55000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:29:  56000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:31:  57000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:32:  58000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:34: tag size is determined as 35 bps \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:34: # tag size = 35 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:34: # total tags in alignment file: 26546343 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:34: # Build Peak Model... \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:34: #2 looking for paired plus/minus strand peaks... \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:38: #2 number of paired peaks: 85709 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:38: start model_add_line... \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:40: start X-correlation... \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:40: end of X-cor \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:40: # finished! \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:40: # predicted fragment length is 109 bps \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:40: # alternative fragment length(s) may be 109 bps \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:40: # Generate R script for model : predictd \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:40: # read alignment files... \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:40: # read treatment tags... \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:40: Detected format is: BAM \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  @ Thu, 10 Dec 2020 12:01:40: * Input file is gzipped. \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:42:  1000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:44:  2000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:45:  3000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:47:  4000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:49:  5000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:51:  6000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:53:  7000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:54:  8000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:56:  9000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:01:58:  10000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:00:  11000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:02:  12000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:03:  13000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:05:  14000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:07:  15000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:09:  16000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:10:  17000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:12:  18000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:14:  19000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:16:  20000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:18:  21000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:19:  22000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:21:  23000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:23:  24000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:25:  25000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:27:  26000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:28:  27000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:30:  28000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:32:  29000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:34:  30000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:35:  31000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:37:  32000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:39:  33000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:41:  34000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:43:  35000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:44:  36000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:46:  37000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:48:  38000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:50:  39000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:51:  40000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:53:  41000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:55:  42000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:57:  43000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:02:59:  44000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:00:  45000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:02:  46000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:04:  47000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:06:  48000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:07:  49000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:09:  50000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:10:  51000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:12:  52000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:13:  53000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:14:  54000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:15: tag size is determined as 35 bps \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:15: # tag size = 35 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:15: # total tags in alignment file: 23470878 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:15: # Build Peak Model... \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:15: #2 looking for paired plus/minus strand peaks... \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:20: #2 number of paired peaks: 100629 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:20: start model_add_line... \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:22: start X-correlation... \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:22: end of X-cor \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:22: # finished! \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:22: # predicted fragment length is 110 bps \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:22: # alternative fragment length(s) may be 110 bps \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:22: # Generate R script for model : predictd \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:22: # read alignment files... \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:22: # read treatment tags... \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:22: Detected format is: BAM \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:22: * Input file is gzipped. \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:24:  1000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:25:  2000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:27:  3000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:29:  4000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:31:  5000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:32:  6000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:34:  7000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:36:  8000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:38:  9000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:40:  10000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:41:  11000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:43:  12000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:45:  13000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:47:  14000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:48:  15000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:50:  16000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:52:  17000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:54:  18000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:56:  19000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:57:  20000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:03:59:  21000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:01:  22000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:03:  23000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:05:  24000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:06:  25000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:08:  26000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:10:  27000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:12:  28000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:13:  29000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:15:  30000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:17:  31000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:19:  32000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:20:  33000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:22:  34000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:24:  35000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:26:  36000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:27:  37000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:29:  38000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:31:  39000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:32:  40000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:34:  41000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:35:  42000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:37:  43000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:38:  44000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:40:  45000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:42:  46000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:43:  47000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:45:  48000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:46:  49000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:48:  50000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:50:  51000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:51:  52000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:53:  53000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:54:  54000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:56:  55000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:57:  56000000 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:59: tag size is determined as 35 bps \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:59: # tag size = 35 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:59: # total tags in alignment file: 25786934 \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:59: # Build Peak Model... \n",
      "INFO  @ Thu, 10 Dec 2020 12:04:59: #2 looking for paired plus/minus strand peaks... \n",
      "INFO  @ Thu, 10 Dec 2020 12:05:03: #2 number of paired peaks: 83529 \n",
      "INFO  @ Thu, 10 Dec 2020 12:05:03: start model_add_line... \n",
      "INFO  @ Thu, 10 Dec 2020 12:05:04: start X-correlation... \n",
      "INFO  @ Thu, 10 Dec 2020 12:05:04: end of X-cor \n",
      "INFO  @ Thu, 10 Dec 2020 12:05:04: # finished! \n",
      "INFO  @ Thu, 10 Dec 2020 12:05:04: # predicted fragment length is 112 bps \n",
      "INFO  @ Thu, 10 Dec 2020 12:05:04: # alternative fragment length(s) may be 112 bps \n",
      "INFO  @ Thu, 10 Dec 2020 12:05:04: # Generate R script for model : predictd \n"
     ]
    }
   ],
   "source": [
    "macs2 predictd -i ATAC_DMSO.bam\n",
    "macs2 predictd -i ATAC_O.bam\n",
    "macs2 predictd -i ATAC_OT.bam\n",
    "macs2 predictd -i ATAC_rebound.bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did some research into the correct macs2 options for peak calling ATAC-seq data. A number of forums on BioStars gave different recommendations. The debate revolves around using the `--nomodel --shift --extsize` options or the `--format BAMPE` option. Eventually, I found the answer on the official Harvard guideline for ATAC-seq data analysis:\n",
    "\n",
    "An important consideration when using MACS2 is deciding which types of alignments should be analyzed and how those alignments should be interpreted. The analysis mode is set by the -f argument. There are two primary options:\n",
    "\n",
    "- Analyze only properly paired alignments, but ignore R2 reads and treat R1 reads as singletons. MACS2 creates a model of the fragment lengths and extends the 3' ends of the R1 reads to the calculated average length. An alternative is to skip this model building and instead extend each read to a specified length (e.g., `--nomodel --extsize 300` for 300bp fragments). The value of the length parameter is usually determined from the average size during library preparation. However, neither of these approaches utilizes the value of paired-end sequencing, which defines both fragment ends.\n",
    "\n",
    "- Analyze only properly paired alignments with `-f BAMPE`. Here, the fragments are defined by the paired alignments' ends, and there is no modeling or artificial extension. Singleton alignments are ignored. This is the preferred option for using only properly paired alignments.\n",
    "\n",
    "A MACS2 developer also noted: _\"If you followed original protocol for ATAC-Seq, you should get Paired-End reads. If so, I would suggest you just use `--format BAMPE` to let MACS2 pileup the whole fragments in general. But if you want to focus on looking for where the 'cutting sites' are, then `--nomodel --shift -100 --extsize 200` should work.\"_\n",
    "\n",
    "Look at the `flagstat` outputs, we have over 90% properly paired reads, so I decided to use the `--format BAMPE` option.\n",
    "\n",
    "The [official Harvard guidelines](https://informatics.fas.harvard.edu/atac-seq-guidelines.html#peak) as of this year actually recommends using `Genrich` instead of `MACS2`. This is because Genrich was designed to be able to run all of the post-alignment steps through peak-calling with one command (removal of mitochondiral reads, removal of PCr duplicates, etc.). Most importantly, Genrich provides an ATAC-seq analysis mode in which, rather than inferring the full fragments from the alignments, intervals are interpreted that are centered on transposase cut sites. MACS2 was originally designed for ChIP-seq analysis and so is not perfectly suited to ATAC-seq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  @ Fri, 11 Dec 2020 00:18:23: \n",
      "# Command line: callpeak -B -t ATAC_O.bam -n O_peaks --format BAMPE\n",
      "# ARGUMENTS LIST:\n",
      "# name = O_peaks\n",
      "# format = BAMPE\n",
      "# ChIP-seq file = ['ATAC_O.bam']\n",
      "# control file = None\n",
      "# effective genome size = 2.70e+09\n",
      "# band width = 300\n",
      "# model fold = [5, 50]\n",
      "# qvalue cutoff = 5.00e-02\n",
      "# The maximum gap between significant sites is assigned as the read length/tag size.\n",
      "# The minimum length of peaks is assigned as the predicted fragment length \"d\".\n",
      "# Larger dataset will be scaled towards smaller dataset.\n",
      "# Range for calculating regional lambda is: 10000 bps\n",
      "# Broad region calling is off\n",
      "# Paired-End mode is on\n",
      " \n",
      "INFO  @ Fri, 11 Dec 2020 00:18:23: #1 read fragment files... \n",
      "INFO  @ Fri, 11 Dec 2020 00:18:23: #1 read treatment fragments... \n",
      "INFO  @ Fri, 11 Dec 2020 00:18:27:  1000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:18:31:  2000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:18:35:  3000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:18:40:  4000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:18:44:  5000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:18:48:  6000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:18:52:  7000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:18:57:  8000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:19:01:  9000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:19:05:  10000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:19:09:  11000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:19:14:  12000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:19:18:  13000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:19:22:  14000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:19:27:  15000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:19:31:  16000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:19:35:  17000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:19:39:  18000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:19:44:  19000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:19:48:  20000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:19:51:  21000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:19:55:  22000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:19:59:  23000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:20:03:  24000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:20:06:  25000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:20:10:  26000000 \n",
      "INFO  @ Fri, 11 Dec 2020 00:20:31: #1 mean fragment size is determined as 163.4 bp from treatment \n",
      "INFO  @ Fri, 11 Dec 2020 00:20:31: #1 fragment size = 163.4 \n",
      "INFO  @ Fri, 11 Dec 2020 00:20:31: #1  total fragments in treatment: 26546343 \n",
      "INFO  @ Fri, 11 Dec 2020 00:20:31: #1 user defined the maximum fragments... \n",
      "INFO  @ Fri, 11 Dec 2020 00:20:31: #1 filter out redundant fragments by allowing at most 1 identical fragment(s) \n",
      "INFO  @ Fri, 11 Dec 2020 00:21:20: #1  fragments after filtering in treatment: 19126680 \n",
      "INFO  @ Fri, 11 Dec 2020 00:21:20: #1  Redundant rate of treatment: 0.28 \n",
      "INFO  @ Fri, 11 Dec 2020 00:21:20: #1 finished! \n",
      "INFO  @ Fri, 11 Dec 2020 00:21:20: #2 Build Peak Model... \n",
      "INFO  @ Fri, 11 Dec 2020 00:21:20: #2 Skipped... \n",
      "INFO  @ Fri, 11 Dec 2020 00:21:20: #3 Call peaks... \n",
      "INFO  @ Fri, 11 Dec 2020 00:21:20: #3 Pre-compute pvalue-qvalue table... \n",
      "INFO  @ Fri, 11 Dec 2020 00:22:38: #3 In the peak calling step, the following will be performed simultaneously: \n",
      "INFO  @ Fri, 11 Dec 2020 00:22:38: #3   Write bedGraph files for treatment pileup (after scaling if necessary)... O_peaks_treat_pileup.bdg \n",
      "INFO  @ Fri, 11 Dec 2020 00:22:38: #3   Write bedGraph files for control lambda (after scaling if necessary)... O_peaks_control_lambda.bdg \n",
      "INFO  @ Fri, 11 Dec 2020 00:22:38: #3   Pileup will be based on sequencing depth in treatment. \n",
      "INFO  @ Fri, 11 Dec 2020 00:22:38: #3 Call peaks for each chromosome... \n",
      "INFO  @ Fri, 11 Dec 2020 02:49:40: #4 Write output xls file... O_peaks_peaks.xls \n",
      "INFO  @ Fri, 11 Dec 2020 02:49:41: #4 Write peak in narrowPeak format file... O_peaks_peaks.narrowPeak \n",
      "INFO  @ Fri, 11 Dec 2020 02:49:41: #4 Write summits bed file... O_peaks_summits.bed \n",
      "INFO  @ Fri, 11 Dec 2020 02:49:41: Done! \n",
      "INFO  @ Fri, 11 Dec 2020 02:49:41: \n",
      "# Command line: callpeak -B -t ATAC_OT.bam -n OT_peaks --format BAMPE\n",
      "# ARGUMENTS LIST:\n",
      "# name = OT_peaks\n",
      "# format = BAMPE\n",
      "# ChIP-seq file = ['ATAC_OT.bam']\n",
      "# control file = None\n",
      "# effective genome size = 2.70e+09\n",
      "# band width = 300\n",
      "# model fold = [5, 50]\n",
      "# qvalue cutoff = 5.00e-02\n",
      "# The maximum gap between significant sites is assigned as the read length/tag size.\n",
      "# The minimum length of peaks is assigned as the predicted fragment length \"d\".\n",
      "# Larger dataset will be scaled towards smaller dataset.\n",
      "# Range for calculating regional lambda is: 10000 bps\n",
      "# Broad region calling is off\n",
      "# Paired-End mode is on\n",
      " \n",
      "INFO  @ Fri, 11 Dec 2020 02:49:41: #1 read fragment files... \n",
      "INFO  @ Fri, 11 Dec 2020 02:49:41: #1 read treatment fragments... \n",
      "INFO  @ Fri, 11 Dec 2020 02:49:46:  1000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:49:50:  2000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:49:54:  3000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:49:58:  4000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:50:03:  5000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:50:07:  6000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:50:11:  7000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:50:15:  8000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:50:20:  9000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:50:24:  10000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:50:28:  11000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:50:32:  12000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:50:37:  13000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:50:41:  14000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:50:45:  15000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:50:49:  16000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:50:54:  17000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:50:58:  18000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:51:02:  19000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:51:06:  20000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:51:10:  21000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:51:15:  22000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:51:19:  23000000 \n",
      "INFO  @ Fri, 11 Dec 2020 02:51:37: #1 mean fragment size is determined as 185.0 bp from treatment \n",
      "INFO  @ Fri, 11 Dec 2020 02:51:37: #1 fragment size = 185.0 \n",
      "INFO  @ Fri, 11 Dec 2020 02:51:37: #1  total fragments in treatment: 23470878 \n",
      "INFO  @ Fri, 11 Dec 2020 02:51:37: #1 user defined the maximum fragments... \n",
      "INFO  @ Fri, 11 Dec 2020 02:51:37: #1 filter out redundant fragments by allowing at most 1 identical fragment(s) \n",
      "INFO  @ Fri, 11 Dec 2020 02:52:21: #1  fragments after filtering in treatment: 21693181 \n",
      "INFO  @ Fri, 11 Dec 2020 02:52:21: #1  Redundant rate of treatment: 0.08 \n",
      "INFO  @ Fri, 11 Dec 2020 02:52:21: #1 finished! \n",
      "INFO  @ Fri, 11 Dec 2020 02:52:21: #2 Build Peak Model... \n",
      "INFO  @ Fri, 11 Dec 2020 02:52:21: #2 Skipped... \n",
      "INFO  @ Fri, 11 Dec 2020 02:52:21: #3 Call peaks... \n",
      "INFO  @ Fri, 11 Dec 2020 02:52:21: #3 Pre-compute pvalue-qvalue table... \n",
      "INFO  @ Fri, 11 Dec 2020 02:53:54: #3 In the peak calling step, the following will be performed simultaneously: \n",
      "INFO  @ Fri, 11 Dec 2020 02:53:54: #3   Write bedGraph files for treatment pileup (after scaling if necessary)... OT_peaks_treat_pileup.bdg \n",
      "INFO  @ Fri, 11 Dec 2020 02:53:54: #3   Write bedGraph files for control lambda (after scaling if necessary)... OT_peaks_control_lambda.bdg \n",
      "INFO  @ Fri, 11 Dec 2020 02:53:54: #3   Pileup will be based on sequencing depth in treatment. \n",
      "INFO  @ Fri, 11 Dec 2020 02:53:54: #3 Call peaks for each chromosome... \n",
      "INFO  @ Fri, 11 Dec 2020 07:36:42: #4 Write output xls file... OT_peaks_peaks.xls \n",
      "INFO  @ Fri, 11 Dec 2020 07:36:42: #4 Write peak in narrowPeak format file... OT_peaks_peaks.narrowPeak \n",
      "INFO  @ Fri, 11 Dec 2020 07:36:43: #4 Write summits bed file... OT_peaks_summits.bed \n",
      "INFO  @ Fri, 11 Dec 2020 07:36:43: Done! \n",
      "INFO  @ Fri, 11 Dec 2020 07:36:43: \n",
      "# Command line: callpeak -B -t ATAC_rebound.bam -n rebound_peaks --format BAMPE\n",
      "# ARGUMENTS LIST:\n",
      "# name = rebound_peaks\n",
      "# format = BAMPE\n",
      "# ChIP-seq file = ['ATAC_rebound.bam']\n",
      "# control file = None\n",
      "# effective genome size = 2.70e+09\n",
      "# band width = 300\n",
      "# model fold = [5, 50]\n",
      "# qvalue cutoff = 5.00e-02\n",
      "# The maximum gap between significant sites is assigned as the read length/tag size.\n",
      "# The minimum length of peaks is assigned as the predicted fragment length \"d\".\n",
      "# Larger dataset will be scaled towards smaller dataset.\n",
      "# Range for calculating regional lambda is: 10000 bps\n",
      "# Broad region calling is off\n",
      "# Paired-End mode is on\n",
      " \n",
      "INFO  @ Fri, 11 Dec 2020 07:36:43: #1 read fragment files... \n",
      "INFO  @ Fri, 11 Dec 2020 07:36:43: #1 read treatment fragments... \n",
      "INFO  @ Fri, 11 Dec 2020 07:36:47:  1000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:36:51:  2000000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  @ Fri, 11 Dec 2020 07:36:55:  3000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:36:59:  4000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:37:04:  5000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:37:08:  6000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:37:12:  7000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:37:16:  8000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:37:20:  9000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:37:25:  10000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:37:29:  11000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:37:33:  12000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:37:37:  13000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:37:42:  14000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:37:46:  15000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:37:50:  16000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:37:54:  17000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:37:58:  18000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:38:02:  19000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:38:05:  20000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:38:09:  21000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:38:13:  22000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:38:16:  23000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:38:20:  24000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:38:24:  25000000 \n",
      "INFO  @ Fri, 11 Dec 2020 07:38:43: #1 mean fragment size is determined as 163.3 bp from treatment \n",
      "INFO  @ Fri, 11 Dec 2020 07:38:43: #1 fragment size = 163.3 \n",
      "INFO  @ Fri, 11 Dec 2020 07:38:43: #1  total fragments in treatment: 25786934 \n",
      "INFO  @ Fri, 11 Dec 2020 07:38:43: #1 user defined the maximum fragments... \n",
      "INFO  @ Fri, 11 Dec 2020 07:38:43: #1 filter out redundant fragments by allowing at most 1 identical fragment(s) \n",
      "INFO  @ Fri, 11 Dec 2020 07:39:29: #1  fragments after filtering in treatment: 16300748 \n",
      "INFO  @ Fri, 11 Dec 2020 07:39:29: #1  Redundant rate of treatment: 0.37 \n",
      "INFO  @ Fri, 11 Dec 2020 07:39:29: #1 finished! \n",
      "INFO  @ Fri, 11 Dec 2020 07:39:29: #2 Build Peak Model... \n",
      "INFO  @ Fri, 11 Dec 2020 07:39:29: #2 Skipped... \n",
      "INFO  @ Fri, 11 Dec 2020 07:39:29: #3 Call peaks... \n",
      "INFO  @ Fri, 11 Dec 2020 07:39:29: #3 Pre-compute pvalue-qvalue table... \n",
      "INFO  @ Fri, 11 Dec 2020 07:40:33: #3 In the peak calling step, the following will be performed simultaneously: \n",
      "INFO  @ Fri, 11 Dec 2020 07:40:33: #3   Write bedGraph files for treatment pileup (after scaling if necessary)... rebound_peaks_treat_pileup.bdg \n",
      "INFO  @ Fri, 11 Dec 2020 07:40:33: #3   Write bedGraph files for control lambda (after scaling if necessary)... rebound_peaks_control_lambda.bdg \n",
      "INFO  @ Fri, 11 Dec 2020 07:40:33: #3   Pileup will be based on sequencing depth in treatment. \n",
      "INFO  @ Fri, 11 Dec 2020 07:40:33: #3 Call peaks for each chromosome... \n",
      "INFO  @ Fri, 11 Dec 2020 09:22:32: #4 Write output xls file... rebound_peaks_peaks.xls \n",
      "INFO  @ Fri, 11 Dec 2020 09:22:32: #4 Write peak in narrowPeak format file... rebound_peaks_peaks.narrowPeak \n",
      "INFO  @ Fri, 11 Dec 2020 09:22:32: #4 Write summits bed file... rebound_peaks_summits.bed \n",
      "INFO  @ Fri, 11 Dec 2020 09:22:33: Done! \n"
     ]
    }
   ],
   "source": [
    "macs2 callpeak -B -t ATAC_DMSO.bam -n DMSO_peaks --format BAMPE\n",
    "macs2 callpeak -B -t ATAC_O.bam -n O_peaks --format BAMPE\n",
    "macs2 callpeak -B -t ATAC_OT.bam -n OT_peaks --format BAMPE\n",
    "macs2 callpeak -B -t ATAC_rebound.bam -n rebound_peaks --format BAMPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced analysis\n",
    "Because by its nature ATAC-seq reveals multiple aspects of transcriptional regulation, the next step of the analysis involves interpretation at four different levels: peak, motif, nucleosome, and TF footprint.\n",
    "\n",
    "## Peaks\n",
    "### Peak differential analysis\n",
    "First I get the read depth for each condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# fragment size is determined as 160 bps\n",
      "# fragments after filtering in treatment: 17309886\n",
      "# fragment size is determined as 163 bps\n",
      "# fragments after filtering in treatment: 19126680\n",
      "# fragment size is determined as 184 bps\n",
      "# fragments after filtering in treatment: 21693181\n",
      "# fragment size is determined as 163 bps\n",
      "# fragments after filtering in treatment: 16300748\n"
     ]
    }
   ],
   "source": [
    "egrep \"fragments after filtering in treatment|fragment size is determined as\" DMSO_peaks_peaks.xls\n",
    "egrep \"fragments after filtering in treatment|fragment size is determined as\" O_peaks_peaks.xls\n",
    "egrep \"fragments after filtering in treatment|fragment size is determined as\" OT_peaks_peaks.xls\n",
    "egrep \"fragments after filtering in treatment|fragment size is determined as\" rebound_peaks_peaks.xls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I run `bdgdiff` for differential peak detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  @ Wed, 16 Dec 2020 15:51:37: Read and build treatment 1 bedGraph... \n",
      "INFO  @ Wed, 16 Dec 2020 15:51:57: Read and build control 1 bedGraph... \n",
      "INFO  @ Wed, 16 Dec 2020 15:52:20: Read and build treatment 2 bedGraph... \n",
      "INFO  @ Wed, 16 Dec 2020 15:52:38: Read and build control 2 bedGraph... \n",
      "INFO  @ Wed, 16 Dec 2020 15:55:07: Write peaks... \n",
      "INFO  @ Wed, 16 Dec 2020 15:55:07: Done \n"
     ]
    }
   ],
   "source": [
    "# osimertinib vs control\n",
    "macs2 bdgdiff \\\n",
    "    --t1 O_peaks_treat_pileup.bdg \\\n",
    "    --c1 O_peaks_control_lambda.bdg \\\n",
    "    --t2 DMSO_peaks_treat_pileup.bdg \\\n",
    "    --c2 DMSO_peaks_control_lambda.bdg \\\n",
    "    --d1 19126680 \\\n",
    "    --d2 17309886 \\\n",
    "    -g 60 \\\n",
    "    -l 160 \\\n",
    "    --o-prefix diff_O_vs_DMSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  @ Wed, 16 Dec 2020 15:58:21: Read and build treatment 1 bedGraph... \n",
      "INFO  @ Wed, 16 Dec 2020 15:58:44: Read and build control 1 bedGraph... \n",
      "INFO  @ Wed, 16 Dec 2020 15:59:09: Read and build treatment 2 bedGraph... \n",
      "INFO  @ Wed, 16 Dec 2020 15:59:28: Read and build control 2 bedGraph... \n",
      "INFO  @ Wed, 16 Dec 2020 16:02:06: Write peaks... \n",
      "INFO  @ Wed, 16 Dec 2020 16:02:06: Done \n"
     ]
    }
   ],
   "source": [
    "# osimertinib + trametinib vs osimertinib\n",
    "macs2 bdgdiff \\\n",
    "    --t1 OT_peaks_treat_pileup.bdg \\\n",
    "    --c1 OT_peaks_control_lambda.bdg \\\n",
    "    --t2 O_peaks_treat_pileup.bdg \\\n",
    "    --c2 O_peaks_control_lambda.bdg \\\n",
    "    --d1 21693181 \\\n",
    "    --d2 19126680 \\\n",
    "    -g 60 \\\n",
    "    -l 170 \\\n",
    "    --o-prefix diff_OT_vs_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  @ Wed, 16 Dec 2020 18:01:57: Read and build treatment 1 bedGraph... \n",
      "INFO  @ Wed, 16 Dec 2020 18:02:20: Read and build control 1 bedGraph... \n",
      "INFO  @ Wed, 16 Dec 2020 18:02:46: Read and build treatment 2 bedGraph... \n",
      "INFO  @ Wed, 16 Dec 2020 18:03:04: Read and build control 2 bedGraph... \n",
      "INFO  @ Wed, 16 Dec 2020 18:05:46: Write peaks... \n",
      "INFO  @ Wed, 16 Dec 2020 18:05:46: Done \n"
     ]
    }
   ],
   "source": [
    "# osimertinib + trametinib vs control\n",
    "macs2 bdgdiff \\\n",
    "    --t1 OT_peaks_treat_pileup.bdg \\\n",
    "    --c1 OT_peaks_control_lambda.bdg \\\n",
    "    --t2 DMSO_peaks_treat_pileup.bdg \\\n",
    "    --c2 DMSO_peaks_control_lambda.bdg \\\n",
    "    --d1 21693181 \\\n",
    "    --d2 17309886 \\\n",
    "    -g 60 \\\n",
    "    -l 170 \\\n",
    "    --o-prefix diff_OT_vs_DMSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  @ Wed, 16 Dec 2020 17:26:16: Read and build treatment 1 bedGraph... \n",
      "INFO  @ Wed, 16 Dec 2020 17:26:33: Read and build control 1 bedGraph... \n",
      "INFO  @ Wed, 16 Dec 2020 17:26:54: Read and build treatment 2 bedGraph... \n",
      "INFO  @ Wed, 16 Dec 2020 17:27:16: Read and build control 2 bedGraph... \n",
      "INFO  @ Wed, 16 Dec 2020 17:29:52: Write peaks... \n",
      "INFO  @ Wed, 16 Dec 2020 17:29:52: Done \n"
     ]
    }
   ],
   "source": [
    "# rebound vs osimertinib + trametinib\n",
    "macs2 bdgdiff \\\n",
    "    --t1 rebound_peaks_treat_pileup.bdg \\\n",
    "    --c1 rebound_peaks_control_lambda.bdg \\\n",
    "    --t2 OT_peaks_treat_pileup.bdg \\\n",
    "    --c2 OT_peaks_control_lambda.bdg \\\n",
    "    --d1 16300748 \\\n",
    "    --d2 21693181 \\\n",
    "    -g 60 \\\n",
    "    -l 170 \\\n",
    "    --o-prefix diff_rebound_vs_OT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motifs\n",
    "In the original publication, the authors used HOMER for MOTIF analysis. Here I use i-CisTarget instead. The authors found that a motif analysis of OT-treated dormant cells versus control cells revealed that the three most significantly enriched motifs were the consensus sites for TEAD family transcription factors, suggesting that the OT-induced epigenetic state is associated with increased TEAD transcription factor binding. I tried to verify this using i-cisTarget to compare the epigenetic states of OT-treated cells vs DMSO cells. The setup and the results of the analysis are given below:\n",
    "\n",
    "![icis-settings](images/OT_vs_DMSO_settings.png)\n",
    "![icis-results](images/OT_vs_DMSO_results.png)\n",
    "\n",
    "The analysis did not find consensus sites for TEAD transcription factors to be significantly enriched. PHF8 was given as the transcription factor associated with the motif with the highest normalised enrichment score. PHF8 is a histone demethylase which induces an EMT-like process by upregulating key EMT transcription factors SNAIL and ZEB1. Both SNA1L and ZEB1 were discussed in the introduction:\n",
    "\n",
    "_\"YAP has been reported to mediate EMT and to directly bind to canonical EMT transcription factors, including SNAIL, SLUG, and ZEB1. The TEAD transcription factors serve as canonical partners for the Hippo pathway effector YAP, which has been associated with resistance to EGFR TKIs in EGFR-mutant NSCLC.\"_\n",
    "\n",
    "The authors of the original paper do not reference or discuss PHF8. The motif enrichment results from i-cisTarget indicate that the epigenetic changes acquired in dormancy also lead to the upregulation of PHF8, which in turn upregulates the key EMT transcription factors that the authors were discussing. This suggests that the OT-induced epigenetic state is associated not only with increased TEAD transcription factor binding, but also with upregulation of PHF8. I could not find anything in the literature relating PHF8 to EMT in dormancy, and so perhaps this could be an interesting research question.\n",
    "\n",
    "As i-CisTarget did not find that consensus sites for TEAD family transcription factors were enriched for OT_vs_DMSO, I instead tried to run a comparative analysis comparing the i-cisTarget analyses for both OT_vs_DMSO and O_vs_DMSO, hoping to find something insightful by comparing the enriched motifs in either analysis (see below). Unfortunately I could not find anything interesting and the enriched motifs were very similar across both conditions, with no obvious motifs significantly more enriched in one analysis.\n",
    "\n",
    "![comparative-analysis](images/comparative-analysis.png)\n",
    "\n",
    "Naturally, I began to question the steps so far, wondering if there may be some error in my analysis. I ran each of the previous steps again and ended up with the same results. To test if TEAD family transcription factors were significantly enriched in OT cells I searched for putative binding sites for TEAD family transcription factors and then used DeepTools to view the heatmap across different conditions. To do this I:\n",
    "\n",
    "1. Searched the JASPAR database for TEAD family transcription factor motifs. These were then downloaded in MEME format (see JASPAR results below).\n",
    "2. Ran the MEME output through FIMO to find putative binding sites in GFF3 format (tutorial given [here](https://www.researchgate.net/publication/49844689_FIMO_Scanning_for_occurrences_of_a_given_motif)).\n",
    "3. Convert the GFF3 file to BED format with `gff2bed` from the BEDOPS kit so it can be used with DeepTools.\n",
    "4. Use this BED file with DeepTools `plotheatmap` to see if there is a difference in accessibility at these binding sites.\n",
    "\n",
    "![JASPAR](images/JASPAR.png)\n",
    "![FIMO](images/FIMO.png)\n",
    "\n",
    "I also used MotifMap to search for TEAD transcription factor binding motifs. MotifMap gives the putative binding sites for the transcription factor and allows the genomic locations to be easily exported as a BED file or to UCSC genome browser. Unfortunately they only had TEAD1, and didn't have TEAD2, TEAD3 or TEAD4.\n",
    "\n",
    "![MotifMap](images/motif-map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepTools analysis using the TEAD1 map from MotifMap is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeMatrix scale-regions \\\n",
    "    -S ATAC_DMSO.bw ATAC_OT.bw \\\n",
    "    -R TEAD-motif.bed \\\n",
    "    -a 2000 \\\n",
    "    -b 2000 \\\n",
    "    -out TEAD-peaks-ATAC.tab.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHeatmap \\\n",
    "    -m TEAD-peaks-ATAC.tab.gz \\\n",
    "    -out TEAD-peaks-ATAC.png \\\n",
    "    --heatmapHeight 15 \\\n",
    "    --refPointLabel peak.center \\\n",
    "    --regionsLabel peaks \\\n",
    "    --plotTitle 'ATAC-seq TEAD motif signal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TEAD peaks](images/TEAD-peaks-ATAC.png)\n",
    "\n",
    "The heatmap shows a small but distinct increase in accessibility at regions which contain the consensus sequence for TEAD1, which fits with the results given in the paper. It is possible that the results I am getting differ from the paper because I am only using one biological replicate for each condition, whereas they have 3 replicates in their analysis. Perhaps re-running this analysis with the replicates would lead to results which are more similar to those in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepTools analysis using all TEAD motifs generated from JASPAR and FIMO is given below. There were problems with using BEDOPS to make the bed file from GFF3 file, so I instead tried to convert it into a BED file from the FIMO tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motif_id\tmotif_alt_id\tsequence_name\tstart\tstop\tstrand\tscore\tp-value\tq-value\tmatched_sequence\n",
      "MA1121.1\tTEAD2\tchr2\t177482\t177494\t+\t17.7685\t6.92e-09\t0.0793\tccacattccagcc\n",
      "MA1121.1\tTEAD2\tchr1_gl000192_random\t246266\t246278\t+\t17.7685\t6.92e-09\t0.0793\tccacattccagcc\n",
      "MA1121.1\tTEAD2\tchr17_ctg5_hap1\t495988\t496000\t+\t17.7685\t6.92e-09\t0.0793\tccacattccagcc\n",
      "MA1121.1\tTEAD2\tchr16\t533498\t533510\t+\t17.7685\t6.92e-09\t0.0793\tccacattccagcc\n",
      "MA1121.1\tTEAD2\tchr12\t591999\t592011\t+\t17.7685\t6.92e-09\t0.0793\tccacattccagcc\n",
      "MA1121.1\tTEAD2\tchr12\t681204\t681216\t+\t17.7685\t6.92e-09\t0.0793\tCCACATTCCAGCC\n",
      "MA1121.1\tTEAD2\tchr20\t684752\t684764\t+\t17.7685\t6.92e-09\t0.0793\tccacattccagcc\n",
      "MA1121.1\tTEAD2\tchr6\t816670\t816682\t+\t17.7685\t6.92e-09\t0.0793\tccacattccagcc\n",
      "MA1121.1\tTEAD2\tchr20\t1456054\t1456066\t+\t17.7685\t6.92e-09\t0.0793\tccacattccagcc\n"
     ]
    }
   ],
   "source": [
    "head -n 10 fimo.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gff2bed < fimo.gff > TEAD_ALL.bed\n",
    "#cut -f1-3 TEAD_ALL.bed > TEAD_all.bed\n",
    "\n",
    "# remove chromosomes which are not standard. E.g. chr6_apd_hap1\n",
    "#awk '$1~/^.{4,5}\\s' TEAD_all.bed > TEAD_all_filtered.bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View processed BED file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1\t36405\t36418\n",
      "chr1\t36407\t36417\n",
      "chr1\t67507\t67519\n",
      "chr1\t103657\t103670\n",
      "chr1\t107865\t107877\n",
      "chr1\t131166\t131179\n",
      "chr1\t131168\t131178\n",
      "chr1\t139997\t140010\n",
      "chr1\t231033\t231046\n",
      "chr1\t249974\t249987\n"
     ]
    }
   ],
   "source": [
    "head -n 10 TEAD_all.bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepTools analysis using the processed BED file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeMatrix scale-regions \\\n",
    "    -S ATAC_DMSO.bw ATAC_OT.bw \\\n",
    "    -R TEAD_all.bed \\\n",
    "    -a 2000 \\\n",
    "    -b 2000 \\\n",
    "    -out TEAD-all-peaks-ATAC.tab.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHeatmap \\\n",
    "    -m TEAD-all-peaks-ATAC.tab.gz \\\n",
    "    -out TEAD-all-peaks-ATAC.png \\\n",
    "    --heatmapHeight 15 \\\n",
    "    --refPointLabel peak.center \\\n",
    "    --regionsLabel peaks \\\n",
    "    --plotTitle 'ATAC-seq TEAD motif signal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used DeepTools' `plotEnrichment` function to calculate the signal enrichment for regions with the TEAD1 motif. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotEnrichment -b ATAC_DMSO.bam ATAC_O.bam ATAC_OT.bam ATAC_rebound.bam \\\n",
    "--BED TEAD-motif.bed \\\n",
    "--labels DMSO O OT REBOUND \\\n",
    "-o enrichment.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISCUSSION\n",
    "The analyses performed above agree with the results presented in the paper in some respects, and don't in others. The DeepTools PCA plots and correlation heatmaps indicate that the OT-treated cells enter a distinct epigenetic state in dormancy. The author's of this paper presented a motif analysis which found that the TEAD-family transcription factor binding sites were more accessible. The TEAD-family transcription factors are known to upregulate canonical EMT transcription factors SNA1L, SLUG and ZEB1, which leads to an increased metastatic potential in dormant NSLSC cells.\n",
    "\n",
    "The motif analysis performed here using i-cisTarget did not find the TEAD-family transcription factors to be the most significantly upregulated, and instead found that consensus binding sites for PHF8 were significantly upregulated. PHF8 is also known to upregulate SNA1L and SLUG, leading to increased metastatic potential. This is interesting and might be something worth looking into. To confirm that consensus binding sites for TEAD-family transcription factors were upregulated, DeepTools plotHeatMap function was used to determine whether chromatin accessibility changes at these binding sites between DMSO cells and OT-treated cells. This confirmed that TEAD was in fact upregulated in these regions.\n",
    "\n",
    "Unfortunately I did not have the time to also run the ChIP-seq analysis that was also a key part of this paper. However I learned much about the ATAC-seq pipeline and tried my best to be as thorough as possible, so I'm happy nonetheless.\n",
    "\n",
    "Merry Christmas!\n",
    "\n",
    "![Christmas](images/chrysler.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
